{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Machine Learning Yield Prediction\n",
        "## Smart Wafer Yield Optimization Project\n",
        "\n",
        "This notebook implements comprehensive machine learning models for semiconductor yield prediction using the SECOM dataset.\n",
        "\n",
        "### Objectives:\n",
        "- Handle class imbalance with advanced techniques\n",
        "- Train multiple ML models (Random Forest, XGBoost, LightGBM, Logistic Regression)\n",
        "- Perform hyperparameter tuning and model selection\n",
        "- Evaluate models with comprehensive metrics\n",
        "- Analyze feature importance and model interpretability\n",
        "- Save best performing model for production use\n",
        "\n",
        "### Models to Implement:\n",
        "1. **Random Forest**: Ensemble method with feature importance\n",
        "2. **XGBoost**: Gradient boosting with advanced regularization\n",
        "3. **LightGBM**: Fast gradient boosting with categorical support\n",
        "4. **Logistic Regression**: Linear baseline model\n",
        "5. **Ensemble**: Voting classifier combining best models\n",
        "\n",
        "### Evaluation Metrics:\n",
        "- Accuracy, Precision, Recall, F1-Score\n",
        "- ROC-AUC and Precision-Recall curves\n",
        "- Confusion Matrix analysis\n",
        "- Cross-validation performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
        "                           roc_curve, precision_recall_curve, accuracy_score, \n",
        "                           precision_score, recall_score, f1_score)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import joblib\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our utility functions\n",
        "import sys\n",
        "import os\n",
        "notebook_path = os.path.abspath(\"\")\n",
        "if notebook_path.endswith(\"notebooks\"):\n",
        "    project_root = os.path.dirname(notebook_path)\n",
        "    os.chdir(project_root)\n",
        "from app.utils import load_data, preprocess_data\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to begin machine learning model development...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Notes for later:\n",
        "| Component                 | Why It Matters (Micron + SECOM Context)                                          |\n",
        "| ------------------------- | -------------------------------------------------------------------------------- |\n",
        "| **Median Imputer**        | Handles missing process sensor readings robustly                                 |\n",
        "| **RobustScaler**          | Minimizes impact of sensor outliers and spikes                                   |\n",
        "| **Yeo–Johnson Transform** | Reduces skew and stabilizes variance (like log but works with negatives)         |\n",
        "| **PCA (50–100)**          | Removes redundancy among 590 correlated sensors, reduces overfitting             |\n",
        "| **XGBoost**               | High accuracy, handles nonlinearity, outliers, imbalance                         |\n",
        "| **SHAP** (post-model)     | Provides interpretability for engineers—shows which sensors contribute to faults |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
        "# from sklearn.decomposition import PCA\n",
        "# from xgboost import XGBClassifier\n",
        "\n",
        "# # Step-by-step balanced pipeline\n",
        "# pipeline = Pipeline([\n",
        "#     ('imputer', SimpleImputer(strategy='median')),\n",
        "#     # ('feature_engineering', OutlierFlagger(features=high_impact_features)), # Study\n",
        "#     ('scaler', RobustScaler()),\n",
        "#     ('power', PowerTransformer(method='yeo-johnson')), # !!!!!! refer back to 02_feature_engineering for some notes\n",
        "#     ('pca', PCA(n_components=50, random_state=42)),  # or tune 30–100\n",
        "#     ('clf', XGBClassifier(\n",
        "#         scale_pos_weight=10,        # adjust for imbalance\n",
        "#         eval_metric='auc',\n",
        "#         random_state=42,\n",
        "#         n_estimators=300,\n",
        "#         max_depth=5,\n",
        "#         learning_rate=0.05\n",
        "#     ))\n",
        "# ])\n",
        "\n",
        "# After training:\n",
        "# import shap\n",
        "\n",
        "# explainer = shap.TreeExplainer(pipeline.named_steps['clf'])\n",
        "# X_pca = pipeline.named_steps['pca'].transform(\n",
        "#     pipeline.named_steps['power'].transform(\n",
        "#         pipeline.named_steps['scaler'].transform(X_test)\n",
        "#     )\n",
        "# )\n",
        "# shap_values = explainer.shap_values(X_pca)\n",
        "# shap.summary_plot(shap_values, X_pca)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the preprocessed data\n",
        "print(\"Loading preprocessed SECOM data...\")\n",
        "import os\n",
        "import time\n",
        "data = load_data()\n",
        "\n",
        "# Check if we have preprocessed data, otherwise preprocess\n",
        "if os.path.exists('../data/processed/secom_cleaned.csv'):\n",
        "    data = pd.read_csv('../data/processed/secom_cleaned.csv')\n",
        "    print(\"✅ Loaded preprocessed data\")\n",
        "else:\n",
        "    print(\"⚠️ No preprocessed data found, preprocessing now...\")\n",
        "    data = preprocess_data(data, method='knn')\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
        "\n",
        "# Separate features and target\n",
        "if 'target' in data.columns:\n",
        "    X = data.drop('target', axis=1)\n",
        "    y = data['target']\n",
        "    print(f\"Features: {X.shape[1]}, Target distribution: {y.value_counts().to_dict()}\")\n",
        "    print(f\"Class balance ratio: {y.value_counts().min() / y.value_counts().max():.3f}\")\n",
        "else:\n",
        "    print(\"❌ No target variable found!\")\n",
        "    X = data\n",
        "    y = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Handle Class Imbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle class imbalance using SMOTE\n",
        "print(\"Handling class imbalance...\")\n",
        "\n",
        "# Split data first\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Original training set distribution: {y_train.value_counts().to_dict()}\")\n",
        "\n",
        "# Apply SMOTE to balance classes\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"After SMOTE distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n",
        "print(f\"Training set size: {X_train_balanced.shape[0]} (was {X_train.shape[0]})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Multiple ML Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models to train\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        random_state=42,\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train_balanced, y_train_balanced)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    \n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'training_time': training_time\n",
        "    }\n",
        "    \n",
        "    print(f\"✅ {name} - Accuracy: {accuracy:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\")\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame({\n",
        "    name: {metric: results[name][metric] for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']}\n",
        "    for name in results.keys()\n",
        "}).T\n",
        "\n",
        "print(\"\\n📊 Model Performance Summary:\")\n",
        "print(results_df.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations for model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Model Performance Comparison\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
        "x_pos = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "for i, (name, result) in enumerate(results.items()):\n",
        "    values = [result[metric] for metric in metrics]\n",
        "    axes[0, 0].bar(x_pos + i*width, values, width, label=name, alpha=0.8)\n",
        "\n",
        "axes[0, 0].set_xlabel('Metrics')\n",
        "axes[0, 0].set_ylabel('Score')\n",
        "axes[0, 0].set_title('Model Performance Comparison')\n",
        "axes[0, 0].set_xticks(x_pos + width/2)\n",
        "axes[0, 0].set_xticklabels(metrics)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. ROC Curves\n",
        "for name, result in results.items():\n",
        "    model = result['model']\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    auc = result['auc']\n",
        "    axes[0, 1].plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
        "\n",
        "axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "axes[0, 1].set_xlabel('False Positive Rate')\n",
        "axes[0, 1].set_ylabel('True Positive Rate')\n",
        "axes[0, 1].set_title('ROC Curves')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Confusion Matrix for best model\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['f1'])\n",
        "best_model = results[best_model_name]['model']\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
        "axes[1, 0].set_title(f'Confusion Matrix - {best_model_name}')\n",
        "axes[1, 0].set_xlabel('Predicted')\n",
        "axes[1, 0].set_ylabel('Actual')\n",
        "\n",
        "# 4. Feature Importance (for Random Forest)\n",
        "if 'Random Forest' in results:\n",
        "    rf_model = results['Random Forest']['model']\n",
        "    feature_importance = rf_model.feature_importances_\n",
        "    \n",
        "    # Get top 20 features\n",
        "    top_features_idx = np.argsort(feature_importance)[-20:]\n",
        "    top_features_importance = feature_importance[top_features_idx]\n",
        "    top_features_names = [f'Feature_{i}' for i in top_features_idx]\n",
        "    \n",
        "    axes[1, 1].barh(range(len(top_features_names)), top_features_importance)\n",
        "    axes[1, 1].set_yticks(range(len(top_features_names)))\n",
        "    axes[1, 1].set_yticklabels(top_features_names)\n",
        "    axes[1, 1].set_xlabel('Importance')\n",
        "    axes[1, 1].set_title('Top 20 Feature Importance')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"F1 Score: {results[best_model_name]['f1']:.3f}\")\n",
        "print(f\"AUC Score: {results[best_model_name]['auc']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "import os\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "best_model = results[best_model_name]['model']\n",
        "model_path = '../models/yield_predictor.pkl'\n",
        "\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f\"✅ Best model ({best_model_name}) saved to {model_path}\")\n",
        "\n",
        "# Save model metadata\n",
        "metadata = {\n",
        "    'model_name': best_model_name,\n",
        "    'accuracy': results[best_model_name]['accuracy'],\n",
        "    'precision': results[best_model_name]['precision'],\n",
        "    'recall': results[best_model_name]['recall'],\n",
        "    'f1': results[best_model_name]['f1'],\n",
        "    'auc': results[best_model_name]['auc'],\n",
        "    'training_time': results[best_model_name]['training_time'],\n",
        "    'n_features': X.shape[1],\n",
        "    'n_samples': X.shape[0]\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../models/model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"✅ Model metadata saved\")\n",
        "print(f\"📊 Final Model Performance:\")\n",
        "print(f\"   Accuracy: {metadata['accuracy']:.3f}\")\n",
        "print(f\"   F1 Score: {metadata['f1']:.3f}\")\n",
        "print(f\"   AUC Score: {metadata['auc']:.3f}\")\n",
        "print(f\"   Features: {metadata['n_features']}\")\n",
        "print(f\"   Samples: {metadata['n_samples']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
