{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Feature Engineering\n",
        "## Smart Wafer Yield Optimization Project\n",
        "\n",
        "This notebook focuses on advanced feature engineering techniques for the SECOM semiconductor manufacturing dataset.\n",
        "\n",
        "### Objectives:\n",
        "- Apply PCA for dimensionality reduction\n",
        "- Create domain-specific features for manufacturing data\n",
        "- Implement statistical feature extraction\n",
        "- Perform feature selection and ranking\n",
        "- Prepare engineered features for machine learning\n",
        "\n",
        "### Feature Engineering Strategies:\n",
        "1. **Dimensionality Reduction**: PCA to reduce 591 features to manageable size\n",
        "2. **Statistical Features**: Rolling statistics, trend analysis, stability metrics\n",
        "3. **Domain Features**: Sensor drift detection, process jump identification\n",
        "4. **Feature Selection**: Mutual information, recursive feature elimination\n",
        "5. **Feature Ranking**: Importance analysis for manufacturing insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest, RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our utility functions\n",
        "import sys\n",
        "import os\n",
        "notebook_path = os.path.abspath(\"\")\n",
        "if notebook_path.endswith(\"notebooks\"):\n",
        "    project_root = os.path.dirname(notebook_path)\n",
        "    os.chdir(project_root)\n",
        "from app.utils import load_data\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to begin feature engineering...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes for later:\n",
        "\n",
        "✅ Step A — Create binary outlier flags for high-impact features\n",
        "\n",
        "For the top features in your “reliable indicators” list (e.g. feature_38, feature_59, feature_348, …), create new binary columns:\n",
        "\n",
        "for f in high_impact_features:\n",
        "    Q1, Q3 = np.percentile(df[f], [25, 75])\n",
        "    IQR = Q3 - Q1\n",
        "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
        "    df[f + '_outlier_flag'] = ((df[f] < lower) | (df[f] > upper)).astype(int)\n",
        "\n",
        "\n",
        "Then include these as new engineered features in your pipeline.\n",
        "These flags encode “abnormal process behavior detected on this sensor” — which is highly interpretable and can improve model recall on faults.\n",
        "\n",
        "\n",
        "✅ Step B — Avoid over-aggressive scaling for these features\n",
        "\n",
        "In your preprocessing, you can:\n",
        "\n",
        "Apply RobustScaler (not StandardScaler), since it won’t squash these few large values.\n",
        "\n",
        "Optionally skip transformation (Yeo–Johnson/log) for these specific features so that their magnitude still carries meaning.\n",
        "\n",
        "Implementation sketch:\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import PowerTransformer, RobustScaler\n",
        "\n",
        "normal_features = [f for f in all_features if f not in high_impact_features]\n",
        "transformer = ColumnTransformer([\n",
        "    ('robust', RobustScaler(), high_impact_features),\n",
        "    ('yeo', PowerTransformer(method='yeo-johnson'), normal_features)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the preprocessed data\n",
        "print(\"Loading preprocessed SECOM data...\")\n",
        "data = load_data()\n",
        "\n",
        "# Check if we have preprocessed data, otherwise preprocess\n",
        "if os.path.exists('../data/processed/secom_cleaned.csv'):\n",
        "    data = pd.read_csv('../data/processed/secom_cleaned.csv')\n",
        "    print(\"✅ Loaded preprocessed data\")\n",
        "else:\n",
        "    print(\"⚠️ No preprocessed data found, using raw data\")\n",
        "    from app.utils import preprocess_data\n",
        "    data = preprocess_data(data, method='knn')\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
        "\n",
        "# Separate features and target\n",
        "if 'target' in data.columns:\n",
        "    X = data.drop('target', axis=1)\n",
        "    y = data['target']\n",
        "    print(f\"Features: {X.shape[1]}, Target distribution: {y.value_counts().to_dict()}\")\n",
        "else:\n",
        "    X = data\n",
        "    y = None\n",
        "    print(\"No target variable found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Principal Component Analysis (PCA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA for dimensionality reduction\n",
        "print(\"Applying PCA for dimensionality reduction...\")\n",
        "\n",
        "# Standardize features first\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Find number of components for 95% variance\n",
        "n_components_95 = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
        "n_components_90 = np.where(cumulative_variance >= 0.90)[0][0] + 1\n",
        "\n",
        "print(f\"Number of components for 90% variance: {n_components_90}\")\n",
        "print(f\"Number of components for 95% variance: {n_components_95}\")\n",
        "\n",
        "# Apply PCA with selected number of components\n",
        "pca_final = PCA(n_components=n_components_95)\n",
        "X_pca_final = pca_final.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original features: {X.shape[1]}\")\n",
        "print(f\"PCA features: {X_pca_final.shape[1]}\")\n",
        "print(f\"Variance explained: {pca_final.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "# Create PCA DataFrame\n",
        "pca_columns = [f'PC_{i+1}' for i in range(X_pca_final.shape[1])]\n",
        "X_pca_df = pd.DataFrame(X_pca_final, columns=pca_columns, index=X.index)\n",
        "\n",
        "# Add target back if available\n",
        "if y is not None:\n",
        "    X_pca_df['target'] = y.values\n",
        "\n",
        "print(\"✅ PCA transformation completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
