{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. CNN Wafer Map Classification\n",
        "## Smart Wafer Yield Optimization Project\n",
        "\n",
        "This notebook implements deep learning techniques using Convolutional Neural Networks (CNN) for wafer map classification and defect pattern recognition.\n",
        "\n",
        "### Objectives:\n",
        "- Generate synthetic wafer map images for demonstration\n",
        "- Build CNN architecture for spatial feature extraction\n",
        "- Implement data augmentation techniques\n",
        "- Train and evaluate CNN model\n",
        "- Visualize activation maps and learned features\n",
        "- Compare CNN performance with traditional ML methods\n",
        "\n",
        "### CNN Architecture:\n",
        "- **Convolutional Layers**: Spatial feature extraction\n",
        "- **Batch Normalization**: Training stability\n",
        "- **Dropout**: Regularization\n",
        "- **Pooling**: Dimensionality reduction\n",
        "- **Fully Connected**: Classification layers\n",
        "\n",
        "### Data Augmentation:\n",
        "- Rotation, flipping, scaling\n",
        "- Noise injection\n",
        "- Brightness/contrast adjustment\n",
        "- Elastic deformation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our utility functions\n",
        "import sys\n",
        "import os\n",
        "notebook_path = os.path.abspath(\"\")\n",
        "if notebook_path.endswith(\"notebooks\"):\n",
        "    project_root = os.path.dirname(notebook_path)\n",
        "    os.chdir(project_root)\n",
        "from app.utils import load_data\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to begin CNN wafer map classification...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Synthetic Wafer Map Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic wafer map images for demonstration\n",
        "def generate_wafer_map(size=64, defect_type='normal'):\n",
        "    \"\"\"\n",
        "    Generate synthetic wafer map images\n",
        "    \n",
        "    Args:\n",
        "        size (int): Size of the wafer map (size x size)\n",
        "        defect_type (str): Type of defect pattern ('normal', 'center', 'edge', 'random', 'cluster')\n",
        "    \n",
        "    Returns:\n",
        "        np.array: Wafer map image\n",
        "    \"\"\"\n",
        "    # Create base wafer (circular)\n",
        "    center = size // 2\n",
        "    y, x = np.ogrid[:size, :size]\n",
        "    distance = np.sqrt((x - center)**2 + (y - center)**2)\n",
        "    wafer_mask = distance <= center\n",
        "    \n",
        "    # Initialize wafer map\n",
        "    wafer_map = np.zeros((size, size))\n",
        "    \n",
        "    if defect_type == 'normal':\n",
        "        # Normal wafer with few random defects\n",
        "        defects = np.random.random((size, size)) < 0.02\n",
        "        wafer_map[defects & wafer_mask] = 1\n",
        "        \n",
        "    elif defect_type == 'center':\n",
        "        # Center defect pattern\n",
        "        center_defect = distance <= 5\n",
        "        wafer_map[center_defect] = 1\n",
        "        \n",
        "    elif defect_type == 'edge':\n",
        "        # Edge defect pattern\n",
        "        edge_defect = distance >= center - 3\n",
        "        wafer_map[edge_defect & wafer_mask] = 1\n",
        "        \n",
        "    elif defect_type == 'random':\n",
        "        # Random scattered defects\n",
        "        defects = np.random.random((size, size)) < 0.1\n",
        "        wafer_map[defects & wafer_mask] = 1\n",
        "        \n",
        "    elif defect_type == 'cluster':\n",
        "        # Cluster defect pattern\n",
        "        cluster_center = (center + np.random.randint(-10, 10), center + np.random.randint(-10, 10))\n",
        "        cluster_distance = np.sqrt((x - cluster_center[0])**2 + (y - cluster_center[1])**2)\n",
        "        cluster_defect = cluster_distance <= 8\n",
        "        wafer_map[cluster_defect & wafer_mask] = 1\n",
        "    \n",
        "    return wafer_map\n",
        "\n",
        "# Generate dataset\n",
        "print(\"Generating synthetic wafer map dataset...\")\n",
        "\n",
        "n_samples = 1000\n",
        "image_size = 64\n",
        "defect_types = ['normal', 'center', 'edge', 'random', 'cluster']\n",
        "n_per_type = n_samples // len(defect_types)\n",
        "\n",
        "X_images = []\n",
        "y_labels = []\n",
        "\n",
        "for defect_type in defect_types:\n",
        "    for _ in range(n_per_type):\n",
        "        # Generate wafer map\n",
        "        wafer_map = generate_wafer_map(image_size, defect_type)\n",
        "        X_images.append(wafer_map)\n",
        "        y_labels.append(defect_types.index(defect_type))\n",
        "\n",
        "X_images = np.array(X_images)\n",
        "y_labels = np.array(y_labels)\n",
        "\n",
        "print(f\"✅ Generated {len(X_images)} wafer maps\")\n",
        "print(f\"Image shape: {X_images.shape}\")\n",
        "print(f\"Label distribution: {np.bincount(y_labels)}\")\n",
        "\n",
        "# Visualize sample wafer maps\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, defect_type in enumerate(defect_types):\n",
        "    # Find first sample of this type\n",
        "    sample_idx = np.where(y_labels == i)[0][0]\n",
        "    axes[i].imshow(X_images[sample_idx], cmap='viridis')\n",
        "    axes[i].set_title(f'{defect_type.capitalize()} Defect')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# Hide unused subplot\n",
        "axes[5].set_visible(False)\n",
        "\n",
        "plt.suptitle('Sample Wafer Map Defect Patterns', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Build CNN Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define CNN architecture for wafer map classification\n",
        "class WaferMapCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(WaferMapCNN, self).__init__()\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "        # Calculate the size after convolutions and pooling\n",
        "        # 64x64 -> 32x32 -> 16x16 -> 8x8\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "        \n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # First conv block\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Second conv block\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # Third conv block\n",
        "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "model = WaferMapCNN(num_classes=5)\n",
        "print(\"✅ CNN model created\")\n",
        "print(f\"Model architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "print(\"Preparing data for CNN training...\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_images, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train).unsqueeze(1)  # Add channel dimension\n",
        "X_test = torch.FloatTensor(X_test).unsqueeze(1)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"✅ Data loaders created\")\n",
        "\n",
        "# Set up training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        train_total += target.size(0)\n",
        "        train_correct += (predicted == target).sum().item()\n",
        "    \n",
        "    # Test phase\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            test_total += target.size(0)\n",
        "            test_correct += (predicted == target).sum().item()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = 100. * train_correct / train_total\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = 100. * test_correct / test_total\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "        print(f'Epoch {epoch:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "print(\"✅ Training completed!\")\n",
        "\n",
        "# Plot training results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(test_losses, label='Test Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Test Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(train_accuracies, label='Train Accuracy')\n",
        "ax2.plot(test_accuracies, label='Test Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Training and Test Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation and Save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model on test set\n",
        "print(\"Evaluating CNN model...\")\n",
        "\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        \n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_pred = np.array(all_predictions)\n",
        "y_true = np.array(all_targets)\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Classification report\n",
        "defect_type_names = ['normal', 'center', 'edge', 'random', 'cluster']\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=defect_type_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=defect_type_names, yticklabels=defect_type_names)\n",
        "plt.title('CNN Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the trained model\n",
        "import os\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "model_path = '../models/cnn_wafer_classifier.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Save model metadata\n",
        "metadata = {\n",
        "    'model_type': 'CNN',\n",
        "    'num_classes': 5,\n",
        "    'class_names': defect_type_names,\n",
        "    'test_accuracy': accuracy,\n",
        "    'image_size': 64,\n",
        "    'num_parameters': total_params,\n",
        "    'training_epochs': num_epochs\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../models/cnn_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"✅ CNN model saved to {model_path}\")\n",
        "print(f\"✅ Model metadata saved\")\n",
        "print(f\"📊 Final CNN Performance:\")\n",
        "print(f\"   Test Accuracy: {accuracy:.3f}\")\n",
        "print(f\"   Parameters: {total_params:,}\")\n",
        "print(f\"   Classes: {len(defect_type_names)}\")\n",
        "\n",
        "print(\"\\n🎯 CNN wafer map classification completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
