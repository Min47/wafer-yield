{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Anomaly Detection\n",
        "## Smart Wafer Yield Optimization Project\n",
        "\n",
        "This notebook implements advanced anomaly detection techniques for identifying defective wafers and process anomalies in semiconductor manufacturing.\n",
        "\n",
        "### Objectives:\n",
        "- Implement multiple anomaly detection algorithms\n",
        "- Visualize anomalies in 2D and 3D space\n",
        "- Analyze anomaly characteristics and patterns\n",
        "- Compare different detection methods\n",
        "- Save trained anomaly detectors for production use\n",
        "\n",
        "### Algorithms to Implement:\n",
        "1. **Isolation Forest**: Tree-based anomaly detection\n",
        "2. **One-Class SVM**: Support vector machine for novelty detection\n",
        "3. **Local Outlier Factor (LOF)**: Density-based anomaly detection\n",
        "4. **Statistical Methods**: Z-score and modified Z-score\n",
        "5. **Ensemble Methods**: Combining multiple detectors\n",
        "\n",
        "### Visualization Techniques:\n",
        "- t-SNE for 2D anomaly visualization\n",
        "- PCA for dimensionality reduction\n",
        "- Anomaly score distributions\n",
        "- Process parameter analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our utility functions\n",
        "import sys\n",
        "import os\n",
        "notebook_path = os.path.abspath(\"\")\n",
        "if notebook_path.endswith(\"notebooks\"):\n",
        "    project_root = os.path.dirname(notebook_path)\n",
        "    os.chdir(project_root)\n",
        "from app.utils import load_data, preprocess_data\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to begin anomaly detection...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the preprocessed data\n",
        "print(\"Loading preprocessed SECOM data...\")\n",
        "import os\n",
        "import time\n",
        "data = load_data()\n",
        "\n",
        "# Check if we have preprocessed data, otherwise preprocess\n",
        "if os.path.exists('../data/processed/secom_cleaned.csv'):\n",
        "    data = pd.read_csv('../data/processed/secom_cleaned.csv')\n",
        "    print(\"‚úÖ Loaded preprocessed data\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No preprocessed data found, preprocessing now...\")\n",
        "    data = preprocess_data(data, method='knn')\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
        "\n",
        "# For anomaly detection, we'll use features only (no target needed)\n",
        "if 'target' in data.columns:\n",
        "    X = data.drop('target', axis=1)\n",
        "    y_true = data['target']  # Keep for evaluation if available\n",
        "    print(f\"Features: {X.shape[1]}, Using target for evaluation: {y_true.value_counts().to_dict()}\")\n",
        "else:\n",
        "    X = data\n",
        "    y_true = None\n",
        "    print(\"No target variable found - unsupervised anomaly detection\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"‚úÖ Data prepared for anomaly detection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implement Multiple Anomaly Detection Algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement multiple anomaly detection algorithms\n",
        "print(\"Implementing anomaly detection algorithms...\")\n",
        "\n",
        "# Define algorithms\n",
        "algorithms = {\n",
        "    'Isolation Forest': IsolationForest(\n",
        "        contamination=0.1,  # Expected proportion of anomalies\n",
        "        random_state=42\n",
        "    ),\n",
        "    'One-Class SVM': OneClassSVM(\n",
        "        nu=0.1,  # Proportion of outliers\n",
        "        kernel='rbf',\n",
        "        gamma='scale'\n",
        "    ),\n",
        "    'Local Outlier Factor': LocalOutlierFactor(\n",
        "        n_neighbors=20,\n",
        "        contamination=0.1\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate each algorithm\n",
        "results = {}\n",
        "\n",
        "for name, algorithm in algorithms.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train algorithm\n",
        "    start_time = time.time()\n",
        "    if name == 'Local Outlier Factor':\n",
        "        # LOF returns -1 for outliers, 1 for inliers\n",
        "        anomaly_labels = algorithm.fit_predict(X_scaled)\n",
        "        anomaly_scores = algorithm.negative_outlier_factor_\n",
        "    else:\n",
        "        # Other algorithms\n",
        "        algorithm.fit(X_scaled)\n",
        "        anomaly_labels = algorithm.predict(X_scaled)\n",
        "        anomaly_scores = algorithm.decision_function(X_scaled)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Convert to binary (1 = normal, 0 = anomaly)\n",
        "    binary_labels = (anomaly_labels == 1).astype(int)\n",
        "    n_anomalies = np.sum(binary_labels == 0)\n",
        "    \n",
        "    results[name] = {\n",
        "        'algorithm': algorithm,\n",
        "        'labels': binary_labels,\n",
        "        'scores': anomaly_scores,\n",
        "        'n_anomalies': n_anomalies,\n",
        "        'anomaly_rate': n_anomalies / len(binary_labels),\n",
        "        'training_time': training_time\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ {name} - Anomalies detected: {n_anomalies} ({n_anomalies/len(binary_labels)*100:.1f}%)\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nüìä Anomaly Detection Summary:\")\n",
        "summary_df = pd.DataFrame({\n",
        "    name: {\n",
        "        'Anomalies': results[name]['n_anomalies'],\n",
        "        'Anomaly Rate': f\"{results[name]['anomaly_rate']*100:.1f}%\",\n",
        "        'Training Time': f\"{results[name]['training_time']:.2f}s\"\n",
        "    }\n",
        "    for name in results.keys()\n",
        "}).T\n",
        "\n",
        "print(summary_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualize Anomalies in 2D Space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce dimensionality for visualization\n",
        "print(\"Reducing dimensionality for visualization...\")\n",
        "\n",
        "# Apply PCA first for efficiency\n",
        "pca = PCA(n_components=50)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Apply t-SNE for 2D visualization\n",
        "print(\"Applying t-SNE...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_2d = tsne.fit_transform(X_pca)\n",
        "\n",
        "print(\"‚úÖ Dimensionality reduction completed\")\n",
        "\n",
        "# Create visualizations for each algorithm\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot for each algorithm\n",
        "for i, (name, result) in enumerate(results.items()):\n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "    \n",
        "    # Create scatter plot\n",
        "    normal_mask = result['labels'] == 1\n",
        "    anomaly_mask = result['labels'] == 0\n",
        "    \n",
        "    axes[row, col].scatter(X_2d[normal_mask, 0], X_2d[normal_mask, 1], \n",
        "                          c='blue', alpha=0.6, s=20, label='Normal')\n",
        "    axes[row, col].scatter(X_2d[anomaly_mask, 0], X_2d[anomaly_mask, 1], \n",
        "                          c='red', alpha=0.8, s=30, label='Anomaly')\n",
        "    \n",
        "    axes[row, col].set_title(f'{name}\\nAnomalies: {result[\"n_anomalies\"]} ({result[\"anomaly_rate\"]*100:.1f}%)')\n",
        "    axes[row, col].set_xlabel('t-SNE 1')\n",
        "    axes[row, col].set_ylabel('t-SNE 2')\n",
        "    axes[row, col].legend()\n",
        "    axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide unused subplot\n",
        "if len(results) < 4:\n",
        "    axes[1, 1].set_visible(False)\n",
        "\n",
        "plt.suptitle('Anomaly Detection Results in 2D Space', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Anomaly visualization completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Best Anomaly Detector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best anomaly detector (Isolation Forest is typically most robust)\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Choose Isolation Forest as the best detector\n",
        "best_algorithm_name = 'Isolation Forest'\n",
        "best_algorithm = results[best_algorithm_name]['algorithm']\n",
        "\n",
        "# Save the model\n",
        "model_path = '../models/anomaly_detector.pkl'\n",
        "joblib.dump(best_algorithm, model_path)\n",
        "\n",
        "# Save the scaler as well\n",
        "scaler_path = '../models/anomaly_scaler.pkl'\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "print(f\"‚úÖ Best anomaly detector ({best_algorithm_name}) saved to {model_path}\")\n",
        "print(f\"‚úÖ Scaler saved to {scaler_path}\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'algorithm_name': best_algorithm_name,\n",
        "    'n_anomalies': results[best_algorithm_name]['n_anomalies'],\n",
        "    'anomaly_rate': results[best_algorithm_name]['anomaly_rate'],\n",
        "    'training_time': results[best_algorithm_name]['training_time'],\n",
        "    'n_features': X.shape[1],\n",
        "    'n_samples': X.shape[0]\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../models/anomaly_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Anomaly detection metadata saved\")\n",
        "print(f\"üìä Final Anomaly Detection Results:\")\n",
        "print(f\"   Algorithm: {metadata['algorithm_name']}\")\n",
        "print(f\"   Anomalies Detected: {metadata['n_anomalies']}\")\n",
        "print(f\"   Anomaly Rate: {metadata['anomaly_rate']*100:.1f}%\")\n",
        "print(f\"   Features: {metadata['n_features']}\")\n",
        "print(f\"   Samples: {metadata['n_samples']}\")\n",
        "\n",
        "print(\"\\nüéØ Anomaly detection pipeline completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
